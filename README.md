# Word2Vec

Этот репозиторий содержит реализацию алгоритма Word2Vec, инструменты для исследования полученных векторных представлений слов. Основное внимание уделено алгоритмам CBOW и Skip-gram. Поддерживает различные параметры предобработки текста и может быть настроен через командную строку.

Word2Vec — это группа алгоритмов машинного обучения, разработанных для создания векторных представлений слов (эмбеддингов) на основе их контекстного использования в больших текстовых корпусах. Основная идея заключается в том, что слова, встречающиеся в сходных контекстах, имеют похожие значения и, следовательно, должны иметь близкие векторные представления.

Ключевые принципы Word2Vec:
* Распределенная семантика: Значение слова определяется его окружением (контекстом).
* Гипотеза непрерывного пространства: Слова могут быть представлены как точки в непрерывном векторном пространстве.
* Сходство векторов: Семантически близкие слова имеют близкие векторные представления.

## Установка

1. Клонируйте репозиторий:
```chatinput
git clone https://github.com/Vasilevykh-M/Word2Vec.git
cd Word2Vec
```
2. Установите необходимые зависимости:
```chatinput
pip install -r requirements.txt
```

## Использование
Параметры командной строки


| Параметр        | Тип   | Значения по умолчанию | Описание                                                              |
|-----------------|-------|-----------------------|-----------------------------------------------------------------------|
| --type_model    | str   | CBOW                  | Метод векторизации: CBOW (Continuous Bag of Words) или SG (Skip-Gram) |
| --input         | str   | обязательный          | Путь к входному TXT файлу с текстами                                  |
| --language      | str   | russian               | Язык текста                                                           |
| --use_stopwords | flag  | False                 | Использовать стоп-слова при обработке текста                          |
| --use_stemming  | flag  | False                 | Использовать стемминг при обработке текста                            |
| --window_size   | int   | 2                     | Размер окна контекста                                                 |
| --embedding_dim | int   | 64                    | Размерность вектора представления слова                               |
| --batch_size    | int   | 8                     | Число экземпляров в батче при обучении                                |
| --epochs        | int   | 200                   | Число эпох при обучении модели                                        |
| --clustering    | flag  | False                 | Выполнять кластеризацию текста                                        |
| --draw          | flag  | False                 | Визуализировать кластеризацию                                         |
| --output        | str   | обязательный          | Путь для сохранения результатов кластеризации                         |

## Алгоритмы Word2Vec

### Continuous Bag-of-Words (CBOW)
Алгоритм CBOW предсказывает целевое слово на основе окружающих его контекстных слов.
Например, для предложения ``` Над головой синее небо ``` и целевого слова ```синее``` (при размере окна 2), контекстными словами будут ```головой``` и ```небо```.

Архитектура CBOW:

* Вход: Контекстные слова (one-hot encoding).
* Скрытый слой: Усреднение векторов контекстных слов.
* Выход: Softmax слой для предсказания целевого слова.

Преимущества CBOW:
* Быстрее обучается на больших корпусах
* Лучше работает с частотными словами
* Стабильнее сходится

Недостатки CBOW:

* Менее точное представление для редких слов, поскольку усреднение контекста может нивелировать специфические черты редкого слова
* Теряет информацию о порядке слов в контексте из-за усреднения векторов контекста
* Может хуже справляться с задачами, где важна точная семантика редких слов

### Skip-gram
Алгоритм Skip-gram инвертирует задачу CBOW — он предсказывает контекстные слова по целевому слову. 
Для того же предложения и целевого слова ```синее```, модель пытается предсказать слова ```головой``` и ```небо```

Архитектура Skip-gram:
* Вход: Целевое слово (one-hot encoding).
* Скрытый слой: Векторное представление целевого слова.
* Выход: Несколько Softmax слоёв для предсказания каждого контекстного слова.

Преимущества Skip-gram:

* Лучше работает с редкими словами
* Создает более качественные представления для специфических терминов
* Эффективнее на небольших корпусах

Недостатки Skip-gram:

* Требует больше времени на обучение, особенно на больших корпусах
* Может быть избыточным для частых слов, так как они встречаются в большом количестве контекстов
## Предобработка текста
Инструмент включает несколько этапов предобработки текста:

1. Токенизация: Разбиение текста на слова с использованием NLTK
2. Приведение к нижнему регистру: Все слова преобразуются к нижнему регистру
3. Удаление не-буквенных символов: Оставляются только слова, состоящие из букв
4. Удаление стоп-слов (опционально): Удаляются общеупотребительные слова
5. Стемминг (опционально): Приведение слов к их основе